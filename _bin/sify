#!/usr/bin/env python2
# -*- coding: utf8 -*-

from argh.helpers import ArghParser
from argh import arg, expects_obj

email = ''.join(map(lambda c: chr(ord(c) - 1), 'zbohmfxvAqsjodfupo/fev'))
template = """#!/bin/bash
#SBATCH --mail-user=""" + email + """
#SBATCH --mail-type=ALL
#SBATCH -o .slurm/{job}.out
#SBATCH -J {job}
#SBATCH -N 1
#SBATCH --ntasks-per-node={ppn}
#SBATCH -t {walltime}
{extra}

. ~/.bashrc
cd $SLURM_SUBMIT_DIR

{cmd}
"""

def get_current_jobs():
    """get the list of all submitted jobs"""
    from sh import whoami
    me = whoami().rstrip('\n')
    from sh import squeue
    raw = squeue('-h', '-Si', '-o', '"%7i"', '-u', me).rstrip('\n')
    return [int(line.rstrip('" ').lstrip('"')) for line in raw.split('\n')]

def generate_slurm(cmd, jobprefix, queue, memory, ppn_default=12, serial=[0]):
    """generate SLURM script for a single job"""
    if not isinstance(cmd, str):
        cmd = '\n'.join(cmd)

    walltime = dict(test='1:00:00', short='24:00:00', medium='72:00:00')[queue]
    job = '%s-%d' % (jobprefix, serial[0])
    serial[0] += 1

    prefix = 'env OMP_NUM_THREADS='
    ppn = int(cmd[len(prefix): cmd.index(' ', len(prefix))]) if cmd.startswith(prefix) else ppn_default
    extra = '' if memory is None else '#SBATCH --mem %d' % (memory * 1024)

    filename = '.slurm/{job}'.format(job=job)
    content = template.format(job=job, walltime=walltime, ppn=ppn, extra=extra, cmd=cmd)
    return filename, content

def chunks(l, n):
    """cut l into chunks of size no larger than n"""
    m = len(l) / n + 1 # number of chunks. +1 guarantees the "no larger" part
    for i in xrange(min(m, len(l))):
        yield l[i::m]

@arg('script', type=str, help='filename of the script to be processed')
@arg('-q', dest='queue', type=str, choices=['test', 'short', 'medium'], help='queue name', default='short')
@arg('-j', dest='jobprefix', type=str, help='job name prefix', default=None)
@arg('-n', dest='number', type=int, help='put n commands in each SLURM script', default=1)
@arg('--memory', type=int, help='required memory in GB', default=None)
@arg('--after', type=str, metavar='# or #min-#max', help='hold execution till the specified job(s) are finished', default=None)
def process(script, queue, jobprefix, number, memory, after):
    """generate SLURM scripts, one for each line of the input file"""
    with open(script) as f:
        lines = [line.rstrip('\n') for line in f
                 if line[0] not in '#\n' and not line.startswith('set -e')]

    if jobprefix is None:
        from pydoc import pager
        text = ('=' * 60 + '\n').join(generate_slurm(cmd, '{jobprefix}', queue, memory)[1] for cmd in chunks(lines, number))
        pager(text)
        return

    from sh import mkdir
    mkdir('-p', '.slurm')

    from os.path import isfile
    filenames = []
    for cmd in chunks(lines, number):
        filename, content = generate_slurm(cmd, jobprefix, queue, memory)
        if isfile(filename):
            assert not isfile(filename + '.out') # never overwrite started jobs

        with open(filename, 'w') as f:
            f.write(content)
        filenames.append(filename)

    sbatch = 'sbatch'
    if after is not None:
        if '-' not in after:
            sbatch += ' --dependency=afterok:' + after
        else:
            amin, amax = map(int, after.split('-'))
            sbatch += ' --dependency=afterok:' + ':'.join(map(str, [i for i in get_current_jobs() if amin <= i <= amax]))

    print ' && '.join([sbatch + ' ' + filename for filename in filenames])

if __name__ == '__main__':
    parser = ArghParser()
    parser.set_default_command(process)
    parser.dispatch()

